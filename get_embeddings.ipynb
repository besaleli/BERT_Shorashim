{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ca96078-8f6d-48ee-9802-0ed6b83c2bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.corp_df import *\n",
    "from libs.Embeddings import *\n",
    "from lexicons.mila import get_lex\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22f308d4-a75e-4389-b2ed-46d4d88ea040",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir corpora/articles_stage2\n",
    "logname_embedding = 'logs/embedding_log.txt'\n",
    "logname_gold = 'logs/gold_annotation.txt'\n",
    "def logInfo(doc_id, num_toks, json_size, time_to_encode, gold_info):\n",
    "    with open(logname_embedding, 'a') as f:\n",
    "        w = lambda i: f.write(i + '\\n')\n",
    "        w('DOC ID: ' + str(doc_id))\n",
    "        w('# TOKS: ' + str(num_toks))\n",
    "        w('JSON SIZE: ' + str(json_size) + 'mb')\n",
    "        w('ENCODING TIME: ' + str(time_to_encode) + 's')\n",
    "        w('')\n",
    "        f.close()\n",
    "        \n",
    "    with open(logname_gold, 'a') as g:\n",
    "        g.write('DOC ID: ' + str(doc_id) + '\\n\\n')\n",
    "        g.write(gold_info + '\\n\\n')\n",
    "        g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b38520f-d2a0-4cb1-9baf-87501b255c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddb7b02ed5c74bb9b9eccabb2fd26ddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4831 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Ambiguous tokens omitted: 34\n",
      "Ambiguous tokens omitted:\n",
      "{'הושט', 'ניצל', 'צוחצח', 'חרש', 'התפלח', 'אוורר', 'הושב', 'פרש', 'חבק', 'שר', 'ניתק', 'בוסס', 'השיב', 'הודח', 'נתפרש', 'השביע', 'הונה', 'הוצל', 'רוקן', 'הסיח', 'התחבר', 'ניחם', 'ניבא', 'גבה', 'פילח', 'עייף', 'הדיח', 'הזיח', 'חולל', 'טח', 'כופף', 'סובב', 'לווה', 'צותת'}\n"
     ]
    }
   ],
   "source": [
    "# retrieve lexicon, delete ambiguous tokens (ie multiple tokens with the same root or the same binyan)\n",
    "vlex, ambiguous_undotted = get_lex()\n",
    "print('# Ambiguous tokens omitted: ' + str(len(ambiguous_undotted)))\n",
    "print('Ambiguous tokens omitted:')\n",
    "print(ambiguous_undotted)\n",
    "\n",
    "# get dictionaries for mapping\n",
    "def getDictionary(cat_to):\n",
    "    get_cat = lambda i: vlex[i].to_list()\n",
    "    \n",
    "    return {i: j for i, j in zip(get_cat('undotted'), get_cat(cat_to))}\n",
    "\n",
    "root_dict = getDictionary('root')\n",
    "binyan_dict = getDictionary('binyan')\n",
    "\n",
    "# function for getting value from dictionary\n",
    "get_root = lambda i: root_dict[i]\n",
    "get_binyan = lambda i: binyan_dict[i]\n",
    "\n",
    "# VB: Verb | BN: Participle | BNT: Participle in construct state\n",
    "verb_tagset = ['VB', 'BN', 'BNT']\n",
    "\n",
    "# verbs that weren't in the dictionary\n",
    "unsuccessful_verbs = set()\n",
    "unsuccessful_count = 0\n",
    "successful_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e632aab-a932-48c9-92cf-6b5a7cfd3b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get raw articles sans embeddings\n",
    "articles_raw_dir = 'corpora/articles_stage1.pickle'\n",
    "\n",
    "with open(articles_raw_dir, 'rb') as f:\n",
    "    raw_docs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ec8281c-e38f-481e-87f5-5e8923204d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "524\n"
     ]
    }
   ],
   "source": [
    "# filter raw docs by size for space efficiency\n",
    "raw_docs_sampled = list([d for d in raw_docs if len(str(d).split()) < 500])\n",
    "print(len(raw_docs_sampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbb2e434-a133-42b8-bee4-0437a81f8e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at onlplab/alephbert-base were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at onlplab/alephbert-base and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# instantiate AlephBERT model\n",
    "alephBERT = Embedding('onlplab/alephbert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "861610b7-8c4a-4c7d-9ac9-46b21c493853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save directory\n",
    "savedir = 'corpora/articles_stage2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5842d21-6668-4489-bb51-c02e91e7fc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_docs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af78604d-6fe5-4bdb-a75e-00bd447162cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9992dee03434e3ba4de5c4106a0279b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/524 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: POSSIBLE ERROR\n",
      "[CLS] והתשובה כנראה לא מצאה חן בעיניה והיא השתמשה בתשובתינו ובמראינו החיצוני , כהצדקה למעצר הילדות עד תום ההליכים ... בתום הדיון השני מצאנו לנכון להסביר ביתר פירוט מה דעתינו על חוקים וקדושתם ... [SEP]\n",
      "\n",
      "ובמראינו\n",
      "\n",
      "Forms:\n",
      "ו\n",
      "ב\n",
      "ה\n",
      "הראה\n",
      "את\n",
      "אנחנו\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# iterate through documents, embedding each sentence\n",
    "for doc in tqdm(raw_docs_sampled):\n",
    "    goldstandard_log = []\n",
    "    for pg in doc:\n",
    "        for sent in pg:\n",
    "            sent.encode_sent(alephBERT)\n",
    "            # get lemmas\n",
    "            sent.lemmatize_sent()\n",
    "            \n",
    "            # check for errors\n",
    "            for tok in sent:\n",
    "                if len(tok.lemmas) > 5:\n",
    "                    print('WARNING: POSSIBLE ERROR')\n",
    "                    print(sent)\n",
    "                    print()\n",
    "                    print(tok.raw_tok)\n",
    "                    print()\n",
    "                    print('Forms:')\n",
    "                    [print(l.form) for l in tok]\n",
    "                    print('\\n\\n')\n",
    "                    \n",
    "                if tok.tokenizer_index is None:\n",
    "                    print(tok.raw_tok)\n",
    "                \n",
    "                # append gold standard verb labels (roots and verbal templates)\n",
    "                for lem in tok:\n",
    "                        if lem.pos_tag in verb_tagset:\n",
    "                            if lem.lemma in root_dict.keys():\n",
    "                                lem.setShoresh(get_root(lem.lemma))\n",
    "                                lem.setBinyan(get_binyan(lem.lemma))\n",
    "                                successful_count += 1\n",
    "                                goldstandard_log.append(lem.lemma + ' <|> ' + get_root(lem.lemma) + ' <|> ' + get_binyan(lem.lemma))\n",
    "                            else:\n",
    "                                lem.setShoresh(None)\n",
    "                                lem.setBinyan(None)\n",
    "                                unsuccessful_verbs.add(lem.lemma)\n",
    "                                unsuccessful_count += 1\n",
    "                                goldstandard_log.append('NOT FOUND')\n",
    "                        else:\n",
    "                            lem.setShoresh(None)\n",
    "                            lem.setBinyan(None)\n",
    "                            goldstandard_log.append('NOT VERB')\n",
    "    \n",
    "    \n",
    "    # new_docs.append(doc)\n",
    "    fName = savedir + str(doc.doc_id) + '.json'\n",
    "    start = time.time()\n",
    "    doc.to_json(fileDir=fName)\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    \n",
    "    # get size info\n",
    "    fsize = os.path.getsize(fName)/(1024 ** 2)\n",
    "    logInfo(doc.doc_id, \n",
    "            len(str(doc).split()), \n",
    "            np.round(fsize, 1), \n",
    "            np.round(elapsed, 2), \n",
    "            '\\n'.join(goldstandard_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11694ce5-a522-4702-a838-5631c0a6035c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold-standard tagging success rate for verbs: 99.92%\n"
     ]
    }
   ],
   "source": [
    "success_rate = np.round(successful_count/(successful_count + unsuccessful_count)*100, 2)\n",
    "print('Gold-standard tagging success rate for verbs: ' + str(success_rate) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ba2cc2-41c3-429c-832b-e8013935ba5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
